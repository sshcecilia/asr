{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune an Automatic Speech Recognition (ASR) AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import cv-valid-train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv-valid-train/sample-000000.mp3</td>\n",
       "      <td>learn to recognize omens and follow them the o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv-valid-train/sample-000001.mp3</td>\n",
       "      <td>everything in the universe evolved he said</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv-valid-train/sample-000002.mp3</td>\n",
       "      <td>you came so that you could learn about your dr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv-valid-train/sample-000003.mp3</td>\n",
       "      <td>so now i fear nothing because it was those ome...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv-valid-train/sample-000004.mp3</td>\n",
       "      <td>if you start your emails with greetings let me...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0  cv-valid-train/sample-000000.mp3   \n",
       "1  cv-valid-train/sample-000001.mp3   \n",
       "2  cv-valid-train/sample-000002.mp3   \n",
       "3  cv-valid-train/sample-000003.mp3   \n",
       "4  cv-valid-train/sample-000004.mp3   \n",
       "\n",
       "                                                text  up_votes  down_votes  \\\n",
       "0  learn to recognize omens and follow them the o...         1           0   \n",
       "1         everything in the universe evolved he said         1           0   \n",
       "2  you came so that you could learn about your dr...         1           0   \n",
       "3  so now i fear nothing because it was those ome...         1           0   \n",
       "4  if you start your emails with greetings let me...         3           2   \n",
       "\n",
       "   age gender accent  duration  \n",
       "0  NaN    NaN    NaN       NaN  \n",
       "1  NaN    NaN    NaN       NaN  \n",
       "2  NaN    NaN    NaN       NaN  \n",
       "3  NaN    NaN    NaN       NaN  \n",
       "4  NaN    NaN    NaN       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('../data/cv-valid-train.csv')\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195776, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training and Validation Set\n",
    "\n",
    "Due to time and resource constraint, I will be using a small subset of the data provided to fine-tune the model.\n",
    "Consequently, the usage of fewer data will lead to lower model performance.\n",
    "\n",
    "The resulting data will then be split into training and validation data using a 70-30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator().manual_seed(1)\n",
    "train_set, val_set = torch.utils.data.random_split(ds[:size], [0.7, 0.3], generator = generator1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "During data preprocessing, audio that are not 16kHz will be resampled. It will then pass through the processor from the pre-trained model to obtain the input values required for fine-tuning. To ensure that the input are of the same shape/ dimension for fine-tuning, the input values will be padded to standardise its length to 180,000. Any longer will be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 50.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 174336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "required_rate = 16000\n",
    "max_len = 0\n",
    "for i in tqdm(train_set.dataset['filename']):\n",
    "    waveform, sample_rate = torchaudio.load(f'../data/cv-valid-train/{i}')\n",
    "    if sample_rate != required_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, required_rate)\n",
    "        resampled_waveform = resampler(waveform)\n",
    "    else:\n",
    "        resampled_waveform = waveform\n",
    "\n",
    "    input_values = processor(resampled_waveform[0], return_tensors=\"pt\", sampling_rate = required_rate, padding = 'do_not_pad').input_values\n",
    "    if len(input_values.squeeze(0)) > max_len:\n",
    "        max_len = len(input_values.squeeze(0))\n",
    "\n",
    "print(f'Maximum length: {max_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(audio_filename):\n",
    "    required_rate = 16000\n",
    "    waveform, sample_rate = torchaudio.load(f'../data/{audio_filename}')\n",
    "    if sample_rate != required_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, required_rate)\n",
    "        resampled_waveform = resampler(waveform)\n",
    "    else:\n",
    "        resampled_waveform = waveform\n",
    "\n",
    "    input_values = processor(resampled_waveform[0], return_tensors=\"pt\", padding=\"max_length\"\n",
    "                             , sampling_rate = required_rate, max_length=180000, truncation = True).input_values\n",
    "    return input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:00<00:00, 63.80it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 60.47it/s]\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "validation_data = []\n",
    "\n",
    "for i in tqdm(train_set.indices):\n",
    "    input = preprocess_input(f'cv-valid-train/{ds.loc[i, 'filename']}').squeeze(0)\n",
    "    output = processor.tokenizer(ds.loc[i, 'text'], return_tensors=\"pt\", padding=\"max_length\", max_length = 200, truncation = True)\n",
    "    training_data.append({'input': input, 'output': output['input_ids'][0]})\n",
    "\n",
    "for i in tqdm(val_set.indices):\n",
    "    input = preprocess_input(f'cv-valid-train/{ds.loc[i, 'filename']}').squeeze(0)\n",
    "    output = processor.tokenizer(ds.loc[i, 'text'], return_tensors=\"pt\", padding=\"max_length\", max_length = 200, truncation = True)\n",
    "    validation_data.append({'input': input, 'output': output['input_ids'][0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 0.706\n",
      "Size of validation data: 0.294\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of training data: {round(len(train_set.indices)/train_set.dataset.shape[0],3)}')\n",
    "print(f'Size of validation data: {round(len(val_set.indices)/train_set.dataset.shape[0],3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(batch_size, num_epochs, learning_rate):\n",
    "\n",
    "    training_dataloader = DataLoader(training_data, batch_size = batch_size, shuffle = True)\n",
    "    validation_dataloader = DataLoader(validation_data, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "    loss_fn = torch.nn.CTCLoss()\n",
    "    min_loss = 10**10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(training_dataloader):\n",
    "            inputs = batch['input']            \n",
    "            labels = batch['output']\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).logits\n",
    "            input_lengths = torch.full((outputs.shape[0],), outputs.shape[1], dtype=torch.long)\n",
    "            target_lengths = torch.full((labels.shape[0],), labels.shape[1], dtype=torch.long)\n",
    "            loss = loss_fn(outputs.transpose(0, 1), labels, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        avg_train_loss = train_loss/len(training_dataloader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(validation_dataloader):\n",
    "                inputs = batch['input']\n",
    "                labels = batch['output']\n",
    "                outputs = model(inputs).logits\n",
    "                input_lengths = torch.full((outputs.shape[0],), outputs.shape[1], dtype=torch.long)\n",
    "                target_lengths = torch.full((labels.shape[0],), labels.shape[1], dtype=torch.long)\n",
    "                loss = loss_fn(outputs.transpose(0, 1), labels, input_lengths, target_lengths)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss/len(validation_dataloader)\n",
    "        \n",
    "        print(epoch)\n",
    "        print(avg_train_loss)\n",
    "        print(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < min_loss:\n",
    "            min_loss = avg_val_loss\n",
    "            best_model = model.state_dict()\n",
    "    \n",
    "    return min_loss, best_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate=1e-05, batch_size=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [19:24<00:00, 64.71s/it]\n",
      "100%|██████████| 8/8 [01:29<00:00, 11.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-13.44194910261366\n",
      "-14.360321521759033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [12:41<00:00, 42.29s/it]\n",
      "100%|██████████| 8/8 [01:24<00:00, 10.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-11.269831551445854\n",
      "-11.981998085975647\n",
      "learning_rate=1e-05, batch_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [41:04<00:00, 308.09s/it]\n",
      "100%|██████████| 3/3 [03:42<00:00, 74.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-8.014303758740425\n",
      "-5.567410945892334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [29:16<00:00, 219.58s/it]\n",
      "100%|██████████| 3/3 [01:35<00:00, 31.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-4.39245143532753\n",
      "-2.1101353963216147\n",
      "learning_rate=0.0001, batch_size=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [22:58<00:00, 76.59s/it]\n",
      "100%|██████████| 8/8 [03:34<00:00, 26.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.4001913805388742\n",
      "-0.23253468051552773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [20:27<00:00, 68.17s/it]\n",
      "100%|██████████| 8/8 [00:53<00:00,  6.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.0771342772576544\n",
      "0.9891796633601189\n",
      "learning_rate=0.0001, batch_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [17:50<00:00, 133.81s/it]\n",
      "100%|██████████| 3/3 [00:51<00:00, 17.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3.307461053133011\n",
      "4.7875870068868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [29:48<00:00, 223.56s/it]\n",
      "100%|██████████| 3/3 [03:06<00:00, 62.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.875274233520031\n",
      "0.8801562984784445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.00001, 0.0001]\n",
    "batch_sizes = [2, 5]\n",
    "num_epochs = 2\n",
    "results = []\n",
    "\n",
    "for i in learning_rates:\n",
    "    for j in batch_sizes:\n",
    "        print(f\"learning_rate={i}, batch_size={j}\")\n",
    "        min_loss, best_model = model_training(learning_rate=i, batch_size=j, num_epochs=num_epochs)\n",
    "        results.append((i, j, min_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate of 1e-05 with batch size of 2 performed best out of the different combinations of hyperparameters tested since it has the lowest loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-05, 2, -14.360321521759033),\n",
       " (1e-05, 5, -5.567410945892334),\n",
       " (0.0001, 2, -0.23253468051552773),\n",
       " (0.0001, 5, 0.8801562984784445)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "final_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "final_model.load_state_dict(best_model)\n",
    "processor.save_pretrained('./wav2vec2-large-960h-cv')\n",
    "processor.tokenizer.save_pretrained('./wav2vec2-large-960h-cv')\n",
    "final_model.save_pretrained('./wav2vec2-large-960h-cv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilize Fine-Tuned Model on cv-valid-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = pd.read_csv('../data/cv-valid-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained('./wav2vec2-large-960h-cv')\n",
    "model = Wav2Vec2ForCTC.from_pretrained('./wav2vec2-large-960h-cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3995 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 504/3995 [00:03<00:28, 121.57it/s]"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "\n",
    "for i in tqdm(range(ds_test.shape[0])):\n",
    "    input = preprocess_input(f'cv-valid-test/{ds_test.loc[i, 'filename']}').squeeze(0)\n",
    "    output = processor.tokenizer(ds_test.loc[i, 'text'], return_tensors=\"pt\", padding=\"max_length\", max_length = 200, truncation = True)\n",
    "    test_data.append({'input': input, 'output': output['input_ids'][0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:11<00:00, 71.72s/it]\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size = 20, shuffle = True)\n",
    "loss_fn = torch.nn.CTCLoss()\n",
    "test_loss = 0\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    inputs = batch['input']\n",
    "    labels = batch['output']\n",
    "    outputs = model(inputs).logits\n",
    "    input_lengths = torch.full((outputs.shape[0],), outputs.shape[1], dtype=torch.long)\n",
    "    target_lengths = torch.full((labels.shape[0],), labels.shape[1], dtype=torch.long)\n",
    "    loss = loss_fn(outputs.transpose(0, 1), labels, input_lengths, target_lengths)\n",
    "    test_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19517859816551208"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Fine-Tuned wav2vec2-large-960h-cv Model with Pre-Trained Model wav2vec2-large-960h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dev = pd.read_csv('../asr/cv-valid-dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename                             cv-valid-dev/sample-000000.mp3\n",
       "text              be careful with your prognostications said the...\n",
       "up_votes                                                          1\n",
       "down_votes                                                        0\n",
       "age                                                             NaN\n",
       "gender                                                          NaN\n",
       "accent                                                          NaN\n",
       "duration                                                        NaN\n",
       "generated_text    BE CAREFUL WITH YOUR PROGNOSTICATIONS SAID THE...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dev.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = preprocess_input(f'cv-valid-dev/{ds_dev.loc[0, 'filename']}')\n",
    "logits = model(input).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
